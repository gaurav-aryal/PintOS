			+--------------------+
       	       	        |    COS 450/550     |
			| PROJECT 1: THREADS |
			|   DESIGN DOCUMENT  |
			+--------------------+
				   
---- GROUP ----

>> Fill in the names and email addresses of your group members.

Gaurav Aryal <gaurav.aryal1@maine.edu>
Gaurav Aryal <gaurav.aryal@maine.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

			     ALARM CLOCK
			     ===========

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

Inside struct thread, there are two new elements:

     extern struct list sleep_list;        /*!< List element for sleeping threads list. */
     int64_t waketick;					  /*!< Thread stores ticks in int64_t type variable. */
	 
As explained in the comment, the first is the element for the sleeping threads list (as
required by that list), and second is the variable to store number of sleep ticks.

---- ALGORITHMS ----

>> A2: Briefly describe what happens in a call to timer_sleep(),
>> including the effects of the timer interrupt handler.

In timer_sleep(), we create a curthread struct for the
current thread, and calculate the earliest wake up time (in ticks).  We
then add this to the global list of sleeping threads, and block the
thread.

>> A3: What steps are taken to minimize the amount of time spent in
>> the timer interrupt handler?

The global list of sleeping threads is ordered, with threads with the
soonest wake-up times at the front. The time interrupt handler only
has to look at the head of the list, check to see if the wake-up time
for that thread has passed, and if it has: wake up that thread, and
traverse the list continuing to wake up threads until we see a thread
whose wakeup time has not passed.  At most, the interrupt handler
examines one thread which does not need to be woken, making the
runtime O(number of threads that need to be woken).

---- SYNCHRONIZATION ----

>> A4: How are race conditions avoided when multiple threads call
>> timer_sleep() simultaneously?

Wherever we traverse, insert into, or delete from our data structures
listed in A1, we disable interrupts, so reads/writes are atomic and
uncorrupted.

>> A5: How are race conditions avoided when a timer interrupt occurs
>> during a call to timer_sleep()?

Interrupts are disabled in the critical sections of timer_sleep, so the
timer interrupt can not occur. This means that no races between the
timer interrupt and the timer_sleep can occur.

---- RATIONALE ----

>> A6: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We first considered storing the wait times in the sleeping thread
structs, but we would have needed to decrement the remaining wait time
at every timer interrupt, an O(n) traversal over
curthreads.  Our implementation instead stores the earliest
possible wakeup time, which is absolute and does not need updates.

			 PRIORITY SCHEDULING
			 ===================

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

In thread.h:

  int basepriority;
  The effective priority of a thread, which takes into account
  priorities donated by other threads.

  struct thread *locker;

  struct list pot_donors;

  struct lock *blocked;


>> B2: Explain the data structure used to track priority donation.
>> Use ASCII art to diagram a nested donation.  (Alternately, submit a
>> .png file.)

   T1     T2 T3     T4
    \     /   \     /
     \   /     \   /
      \ /       \ /    - T1 and T2 are waiting on L1
       L1        L2    - T2 and T3 are waiting on L2
       |         |   
       T5        T6    -L1 is held by T5, L2 by T6
        \        /
         \      /
          \    /
           \  /
            L3         - T5 and T6 are waiting on L3
            |
            T7         - L3 is held by T7


We maintain a directed forest over the locks and threads to track
priority donation. For example, T1 and T2 store L1 in their
blocked field (and T3, T4 store L2). L1 stores T1, T2 in its
waiters list, and T5 in its lock field. The
basepriority field is maintained to be the correct priority using
the information provided by the graph across all possible operations that
might change a thread's donated priority: acquiring a lock, releasing a
lock, and changing a thread's priority.

---- ALGORITHMS ----

>> B3: How do you ensure that the highest priority thread waiting for
>> a lock, semaphore, or condition variable wakes up first?

Change the waiters list to a sorted list, which is ordering by priority.
Every time waking up the waiter, threads are put to ready list from the
beginning to the end, which is actually from the highest priority to a 
the lowest priority.

>> B4: Describe the sequence of events when a call to lock_acquire()
>> causes a priority donation.  How is nested donation handled?

In lock_acquire, if a thread starts waiting on a lock, we check
whether its priority is higher than the effective priority of that
lock's holder. If it is, we call set_basepriority(lock's holder,
priority of current thread). We handle nested donation by having
set_basepriority recursively update the effective priority
through the current lock-thread tree.  We check to see if the lock's
holder is waiting on any locks. If the thread is waiting, we will call
set_basepriority on that lock's holder if its priority should be
higher.

>> B5: Describe the sequence of events when lock_release() is called
>> on a lock that a higher-priority thread is waiting for.

We remove the lock from the list of locks held by the current thread,
and recalculate the effective priority.  The effective priority
calculation takes the max of the thread's original priority
(non-donated), and any priority donations from locks it holds
(excluding the lock we just removed from its holdings list). Finally,
we actually release the lock.

---- SYNCHRONIZATION ----

>> B6: Describe a potential race in thread_set_priority() and explain
>> how your implementation avoids it.  Can you use a lock to avoid
>> this race?

During priority donation, the lock holder’s priority may be set by it’s donor,
at the mean time, the thread itself may want to change the priority.
If the donor and the thread itself set the priority in a different order, may 
cause a different result. 
 
We disable the interrupt to prevent it happens. It can not be avoided using a lock
in our implementation, since we didn’t provide the interface and structure to 
share a lock between donor and the thread itself. If we add a lock to the thread 
struct, it may be avoided using it.

---- RATIONALE ----

>> B7: Why did you choose this design?  In what ways is it superior to
>> another design you considered?

We use the changes of basepriority to indicate 
whether or not a thread’s priority is donated. For example, we initialize 
basepriority to -1 when it is created. Whenever a donation happens, we set it
to the thread’s priority just before donation happening. We compare the value of 
priority and basepriority -- if they are different and basepriority has 
been changed from -1 to other value, then it means a donation happened. Thus, 
basepriority has two functions: 1. store the priority value just before 
donation; 2. indicate whether or not a donation happened. There will be a problem 
when priority lower involved: if the new priority is higher than the current 
donated priority, both priority  and basepriority need to change 
to the new priority. 
 
For struct lock, instead of adding a new memeber in lock, we can get the highest 
priority in its waiters list by doing this 
list_entry (list_front (lock->semaphore.waiters), struct thread, elem)->priority. 
But the code will be more complex.
 
For multiple donations, after the thread releases one lock, its priority should be
 the highest thread priority in the next lock’s waiters
In order to achieve multiple donations, we need to keep track of the highest 
priority for that lock. At the beginning, instead of using a lock list inside of 
struct thread, we thought it would make the list smaller if we keep track of the 
highest-priority thread in the lock’s waiters list. But it did not work because 
after a while, another higher-priority thread acquires this lock, we need to 
replace the thread (or priority) just put in the list. We can not find that thread
(or priority) from the list. Replacement can not be done.

			  ADVANCED SCHEDULER
			  ==================

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

extern bool thread_mlfqs;

The following fields were added to the thread struct.
  /* 4.4BSD scheduling data */
  int nice;
  int recent_cpu;

The nice field stores the current nice value of the thread.

The recent_cpu field stores a value that approximates how much the
thread has been using the CPU recently.

---- ALGORITHMS ----

>> C2: Suppose threads A, B, and C have nice values 0, 1, and 2.  Each
>> has a recent_cpu value of 0.  Fill in the table below showing the
>> scheduling decision and the priority and recent_cpu values for each
>> thread after each given number of timer ticks:

timer  recent_cpu      priority        thread
ticks   A   B   C     A      B     C   to run
-----  --  --   --   ---   ---   ---   ------
0      0   0    0   63.00 61.00 59.00   A
4      4   0    0   62.00 61.00 59.00   A
8      8   0    0   61.00 61.00 59.00   A
12     12  0    0   60.00 61.00 59.00   B
16     12  4    0   60.00 60.00 59.00   B
20     12  8    0   60.00 59.00 59.00   A
24     16  8    0   59.00 59.00 59.00   A
28     20  8    0   58.00 59.00 59.00   C
32     20  8    4   58.00 59.00 58.00   B
36     20  12   4   58.00 58.00 58.00   B

>> C3: Did any ambiguities in the scheduler specification make values
>> in the table uncertain?  If so, what rule did you use to resolve
>> them?  Does this match the behavior of your scheduler?

recent_cpu here is ambiguous here. When we calculate recent_cpu, we did not 
consider the time that CPU spends on the calculations every 4 ticks, like 
load_avg, recent_cpu for all threads, priority for all threads in all_list, 
resort the ready_list. When CPU does these calculations, the current thread needs 
to yield, and not running. Thus, every 4 ticks, the real ticks that is added to 
recent_cpu (recent_cpu is added 1 every ticks) is not really 4 ticks -- less than 
4 ticks. But we could not figure out how much time it spends. What we did was 
adding 4 ticks to recent_cpu every 4 ticks.

>> C4: How is the way you divided the cost of scheduling between code
>> inside and outside interrupt context likely to affect performance?

Though the design doc specifies updating all threads' priorities every
four ticks, we only update the current thread's priority, since the
factors that influence the priorities of other threads cannot change
until the load average changes. This implementation improves
performance by reducing the amount of time in timer interrupts.

Most of our MLFQS scheduling (updating statistics and prioirties)
happens in an interrupt context. The only code that potentially runs
outside an interrupt context involves selecting the next thread to run,
which is at most a small constant number of operations (see below for
more details). This means that a thread can not lose time due to 
scheduler bookkeeping. This prevents the scheduler from stealing time
from a thread, however, lowering the performance.

---- RATIONALE ----

>> C5: Briefly critique your design, pointing out advantages and
>> disadvantages in your design choices.  If you were to have extra
>> time to work on this part of the project, how might you choose to
>> refine or improve your design?

Our design uses an array of ready lists for each possible priority.
Since the library list implementation is a doubly linked, this means
that insertion into and removal from a ready list are constant time
operations. We maintain a count of the number of threads in the MLFQS so
that computing the load average does not require a linear time
operation. The difficulty with this is that we must remember to update
this value every time a thread changes to or from the THREAD_READY
state. 

We implemented the MLFQS system in parallel with the original priority
scheduling system. We did this because we wanted to reduce the
likelihood of introducing bugs into the first scheduler. We switch
between the two systems at only a few key points in the code that relate
to scheduling: thread_unblock, thread_yield, next_thread_to_run.
Unfortunately we could not separate the two systems entirely. In order
to prevent priority donation from happening under MLFQS, we do not
update effective priorities when thread_mlfqs is set.  This is
implemented simply as a return case in a single function
(thread_set_effective_priority).  On the other hand, we actually want
the influence of yielding based on priorities in the case of locks.
Because of this, our MLFQS system keeps a thread's effective_priority
and its mlfqs_priority the same.

Given additional time we might maintain a variable that stores the
highest priority of any thread at any given moment. This would allow us
to find the next thread during scheduling slightly faster. As it stands
now, finding the next thread involves checking all of the ready lists in
decreasing order of priority until a thread is found. 

>> C6: The assignment explains arithmetic for fixed-point math in
>> detail, but it leaves it open to you to implement it.  Why did you
>> decide to implement it the way you did?  If you created an
>> abstraction layer for fixed-point math, that is, an abstract data
>> type and/or a set of functions or macros to manipulate fixed-point
>> numbers, why did you do so?  If not, why not?

As mentioned in the BSD scheduling manual, recent_cpu and load_avg are real 
numbers, but pintos disabled float numbers. Instead using float number, we can 
use fixed-point numbers. So we use fixed-point numbers to represent recent_cpu 
and load_avg.
 
We used #define macro in the new created header fixed-point.h under thread. We 
did not implement them as inline functions in thread.c because
1. They are simple. Every parameter only appears once in the calculation, which 
can avoid the #define macro’s error, which calculate the same parameter 
(expression) multiple times.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students, either for future quarters or the remaining projects?

>> Any other comments?
